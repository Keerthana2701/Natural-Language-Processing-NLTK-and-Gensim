{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Million News Headlines Dataset -Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186018"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to perform lemmatize and stem preprocessing steps on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim.utils.simple_preprocess\n",
    "\n",
    "-Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a index from the document to check if it worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['a', 'g', 'calls', 'for', 'infrastructure', 'protection', 'summit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['call', 'infrastructur', 'protect', 'summit']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 2].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pass the headline_text of documents to the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [decid, commun, broadcast, licenc]\n",
       "1                        [wit, awar, defam]\n",
       "2    [call, infrastructur, protect, summit]\n",
       "3               [staff, aust, strike, rise]\n",
       "4      [strike, affect, australian, travel]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words on the Data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x26594392e48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 commun\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Gensim filter_extremes\n",
    " \n",
    " \n",
    " Filter out tokens that appear in\n",
    " \n",
    " \n",
    "less than 10 documents (absolute number) or\n",
    "\n",
    "\n",
    "more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "\n",
    "\n",
    "after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim doc2bow\n",
    "\n",
    "Convert document into the bag-of-words (BoW) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1), (8, 1), (9, 1), (10, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 7 (\"call\") appears 1 time.\n",
      "Word 8 (\"infrastructur\") appears 1 time.\n",
      "Word 9 (\"protect\") appears 1 time.\n",
      "Word 10 (\"summit\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bow_doc_2 = bow_corpus[2]\n",
    "\n",
    "for i in range(len(bow_doc_2)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_2[i][0], \n",
    "                                                     dictionary[bow_doc_2[i][0]], \n",
    "                                                     bow_doc_2[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5860586362613176),\n",
      " (1, 0.3854657616151764),\n",
      " (2, 0.5006618583937537),\n",
      " (3, 0.5072367544211179)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using Bag of Words\n",
    "\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
    "\n",
    "print_topics - Alias for show_topics() that prints the topn most probable words for topics number of topics to log. Set topics=-1 to print all topics.\n",
    "\n",
    "\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.033*\"queensland\" + 0.022*\"perth\" + 0.018*\"island\" + 0.017*\"speak\" + 0.016*\"interview\" + 0.014*\"releas\" + 0.013*\"assault\" + 0.013*\"prison\" + 0.013*\"stori\" + 0.012*\"scott\"\n",
      "Topic: 1 \n",
      "Words: 0.048*\"australia\" + 0.034*\"trump\" + 0.023*\"world\" + 0.017*\"donald\" + 0.014*\"test\" + 0.013*\"final\" + 0.012*\"miss\" + 0.012*\"women\" + 0.011*\"win\" + 0.010*\"guilti\"\n",
      "Topic: 2 \n",
      "Words: 0.026*\"kill\" + 0.023*\"china\" + 0.022*\"south\" + 0.021*\"news\" + 0.020*\"north\" + 0.018*\"tasmania\" + 0.016*\"australian\" + 0.012*\"west\" + 0.012*\"talk\" + 0.011*\"say\"\n",
      "Topic: 3 \n",
      "Words: 0.023*\"market\" + 0.016*\"water\" + 0.014*\"rise\" + 0.014*\"bank\" + 0.013*\"price\" + 0.012*\"victoria\" + 0.012*\"fall\" + 0.012*\"australian\" + 0.011*\"drum\" + 0.011*\"million\"\n",
      "Topic: 4 \n",
      "Words: 0.033*\"year\" + 0.027*\"court\" + 0.018*\"murder\" + 0.018*\"face\" + 0.016*\"jail\" + 0.015*\"accus\" + 0.014*\"brisban\" + 0.014*\"famili\" + 0.014*\"peopl\" + 0.014*\"alleg\"\n",
      "Topic: 5 \n",
      "Words: 0.034*\"elect\" + 0.024*\"crash\" + 0.021*\"canberra\" + 0.020*\"die\" + 0.015*\"bushfir\" + 0.013*\"commun\" + 0.012*\"darwin\" + 0.011*\"liber\" + 0.010*\"work\" + 0.010*\"rescu\"\n",
      "Topic: 6 \n",
      "Words: 0.022*\"say\" + 0.021*\"live\" + 0.019*\"protest\" + 0.013*\"deal\" + 0.011*\"feder\" + 0.011*\"farmer\" + 0.011*\"reveal\" + 0.010*\"media\" + 0.009*\"report\" + 0.009*\"law\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"govern\" + 0.017*\"health\" + 0.014*\"rural\" + 0.014*\"countri\" + 0.013*\"fund\" + 0.013*\"indigen\" + 0.013*\"state\" + 0.013*\"power\" + 0.012*\"street\" + 0.011*\"labor\"\n",
      "Topic: 8 \n",
      "Words: 0.020*\"chang\" + 0.015*\"busi\" + 0.015*\"council\" + 0.015*\"coast\" + 0.014*\"break\" + 0.013*\"gold\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.011*\"john\" + 0.011*\"plan\"\n",
      "Topic: 9 \n",
      "Words: 0.057*\"polic\" + 0.028*\"death\" + 0.024*\"sydney\" + 0.022*\"nation\" + 0.022*\"charg\" + 0.019*\"shoot\" + 0.017*\"woman\" + 0.015*\"adelaid\" + 0.014*\"arrest\" + 0.014*\"attack\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.032*\"trump\" + 0.009*\"miss\" + 0.009*\"christma\" + 0.008*\"search\" + 0.007*\"septemb\" + 0.006*\"plane\" + 0.005*\"adelaid\" + 0.005*\"beach\" + 0.005*\"island\" + 0.005*\"johnson\"\n",
      "Topic: 1 Word: 0.019*\"market\" + 0.011*\"royal\" + 0.009*\"street\" + 0.009*\"share\" + 0.008*\"turnbul\" + 0.008*\"john\" + 0.008*\"wednesday\" + 0.008*\"busi\" + 0.007*\"china\" + 0.007*\"commiss\"\n",
      "Topic: 2 Word: 0.018*\"charg\" + 0.016*\"murder\" + 0.013*\"polic\" + 0.013*\"court\" + 0.011*\"interview\" + 0.010*\"stori\" + 0.010*\"jail\" + 0.010*\"alleg\" + 0.009*\"woman\" + 0.009*\"guilti\"\n",
      "Topic: 3 Word: 0.008*\"bushfir\" + 0.007*\"financ\" + 0.006*\"million\" + 0.006*\"ash\" + 0.006*\"april\" + 0.006*\"award\" + 0.005*\"western\" + 0.005*\"onlin\" + 0.005*\"coast\" + 0.005*\"firefight\"\n",
      "Topic: 4 Word: 0.013*\"donald\" + 0.008*\"farm\" + 0.007*\"drought\" + 0.006*\"grandstand\" + 0.006*\"peter\" + 0.006*\"water\" + 0.005*\"farmer\" + 0.005*\"plan\" + 0.005*\"fund\" + 0.005*\"novemb\"\n",
      "Topic: 5 Word: 0.012*\"drum\" + 0.009*\"tuesday\" + 0.009*\"monday\" + 0.009*\"flood\" + 0.008*\"violenc\" + 0.008*\"live\" + 0.008*\"sexual\" + 0.008*\"cattl\" + 0.008*\"queensland\" + 0.008*\"mother\"\n",
      "Topic: 6 Word: 0.008*\"thursday\" + 0.007*\"care\" + 0.007*\"zealand\" + 0.006*\"andrew\" + 0.006*\"dairi\" + 0.006*\"octob\" + 0.006*\"age\" + 0.005*\"hong\" + 0.005*\"kong\" + 0.005*\"liber\"\n",
      "Topic: 7 Word: 0.011*\"world\" + 0.009*\"australia\" + 0.009*\"final\" + 0.007*\"leagu\" + 0.007*\"scott\" + 0.006*\"cricket\" + 0.006*\"beat\" + 0.006*\"david\" + 0.005*\"rugbi\" + 0.005*\"test\"\n",
      "Topic: 8 Word: 0.015*\"rural\" + 0.015*\"countri\" + 0.014*\"news\" + 0.013*\"govern\" + 0.011*\"hour\" + 0.010*\"nation\" + 0.008*\"climat\" + 0.007*\"chang\" + 0.007*\"michael\" + 0.007*\"elect\"\n",
      "Topic: 9 Word: 0.017*\"crash\" + 0.014*\"kill\" + 0.012*\"driver\" + 0.010*\"polic\" + 0.010*\"dead\" + 0.010*\"friday\" + 0.009*\"die\" + 0.008*\"shoot\" + 0.008*\"juli\" + 0.007*\"fatal\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
