{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis\n",
    "\n",
    "LSA is a technique of analysing relationships between a set of documents and the terms they contain by producing  a set of concepts\n",
    "related to  the documents and terms\n",
    "\n",
    "We use Singular value decomposition (SVD) property\n",
    "Input decomposed to 3 matrix\n",
    "![alt text](svd.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"The amount of polution is increasing day by day\",\n",
    "           \"The concert was just great\",\n",
    "           \"I love to see Gordon Ramsay cook\",\n",
    "           \"Google is introducing a new technology\",\n",
    "           \"AI Robots are examples of great technology present today\",\n",
    "           \"All of us were singing in the concert\",\n",
    "           \"We have launch campaigns to stop pollution and global warming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [line.lower() for line in dataset] # convert all sentences to low case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the amount of polution is increasing day by day',\n",
       " 'the concert was just great',\n",
       " 'i love to see gordon ramsay cook',\n",
       " 'google is introducing a new technology',\n",
       " 'ai robots are examples of great technology present today',\n",
       " 'all of us were singing in the concert',\n",
       " 'we have launch campaigns to stop pollution and global warming']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Tfidf Model to get features names. it also gives which column corresponds to which word\n",
    "# converts the dataset to tfidf model\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.3211483974289089\n",
      "  (0, 9)\t0.6422967948578178\n",
      "  (0, 17)\t0.3211483974289089\n",
      "  (0, 19)\t0.2665807498646048\n",
      "  (0, 26)\t0.3211483974289089\n",
      "  (0, 24)\t0.2278643877752444\n",
      "  (0, 2)\t0.3211483974289089\n",
      "  (0, 34)\t0.2278643877752444\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the Tfidf Model\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=4, n_iter=100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the SVD  - decompose tfidf matrix to 3 matrix of SVD\n",
    "\n",
    "lsa = TruncatedSVD(n_components = 4, n_iter = 100)   # n-components is no of concepts we need to find\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Column of V\n",
    "\n",
    "col1 = lsa.components_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.39508717e-01,  5.64466475e-02,  2.32713384e-01,  5.38463701e-16,\n",
       "       -2.39508717e-01,  2.32713384e-01,  3.71853138e-16, -4.43102940e-02,\n",
       "       -1.27012912e-16,  4.65426768e-01, -2.39508717e-01,  3.71853138e-16,\n",
       "        2.34583657e-02, -1.27012912e-16, -2.89978663e-01,  3.71853138e-16,\n",
       "        5.64466475e-02,  2.32713384e-01,  2.34583657e-02,  2.12644552e-01,\n",
       "       -1.09827021e-01,  3.71853138e-16, -1.27012912e-16,  2.34583657e-02,\n",
       "        3.52290920e-02,  3.71853138e-16,  2.32713384e-01, -2.39508717e-01,\n",
       "       -1.27012912e-16, -2.39508717e-01, -1.27012912e-16,  5.64466475e-02,\n",
       "        3.71853138e-16, -1.79340346e-01,  1.27242132e-01,  1.95674223e-16,\n",
       "       -2.39508717e-01,  5.64466475e-02,  3.71853138e-16, -1.09827021e-01,\n",
       "        3.71853138e-16,  5.64466475e-02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1  # we get values for all 42 words. high value words are in this concept and others are not in this concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai',\n",
       " 'all',\n",
       " 'amount',\n",
       " 'and',\n",
       " 'are',\n",
       " 'by',\n",
       " 'campaigns',\n",
       " 'concert',\n",
       " 'cook',\n",
       " 'day',\n",
       " 'examples',\n",
       " 'global',\n",
       " 'google',\n",
       " 'gordon',\n",
       " 'great',\n",
       " 'have',\n",
       " 'in',\n",
       " 'increasing',\n",
       " 'introducing',\n",
       " 'is',\n",
       " 'just',\n",
       " 'launch',\n",
       " 'love',\n",
       " 'new',\n",
       " 'of',\n",
       " 'pollution',\n",
       " 'polution',\n",
       " 'present',\n",
       " 'ramsay',\n",
       " 'robots',\n",
       " 'see',\n",
       " 'singing',\n",
       " 'stop',\n",
       " 'technology',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'us',\n",
       " 'warming',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the concepts  - for each concept which are the important key words\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0 :\n",
      "('the', 0.3760982952926375)\n",
      "('concert', 0.34498873923306606)\n",
      "('great', 0.3001240258948742)\n",
      "('of', 0.29579806095266675)\n",
      "('just', 0.23736582929791253)\n",
      "('was', 0.23736582929791253)\n",
      "('day', 0.2289215954150452)\n",
      "('technology', 0.1838383456741344)\n",
      "('all', 0.17824025175628952)\n",
      "('in', 0.17824025175628952)\n",
      "\n",
      "Concept 1 :\n",
      "('to', 0.41578844396700687)\n",
      "('cook', 0.2835916579351076)\n",
      "('gordon', 0.2835916579351076)\n",
      "('love', 0.2835916579351076)\n",
      "('ramsay', 0.2835916579351076)\n",
      "('see', 0.2835916579351076)\n",
      "('and', 0.21730644711292435)\n",
      "('campaigns', 0.21730644711292435)\n",
      "('global', 0.21730644711292435)\n",
      "('have', 0.21730644711292435)\n",
      "\n",
      "Concept 2 :\n",
      "('technology', 0.3779180676714397)\n",
      "('is', 0.3419614380631988)\n",
      "('google', 0.3413969441909746)\n",
      "('introducing', 0.3413969441909746)\n",
      "('new', 0.3413969441909746)\n",
      "('day', 0.141124326809948)\n",
      "('are', 0.11387892195373003)\n",
      "('examples', 0.11387892195373003)\n",
      "('present', 0.11387892195373003)\n",
      "('robots', 0.11387892195373003)\n",
      "\n",
      "Concept 3 :\n",
      "('day', 0.46542676790411075)\n",
      "('amount', 0.23271338395205549)\n",
      "('by', 0.23271338395205537)\n",
      "('increasing', 0.23271338395205537)\n",
      "('polution', 0.23271338395205537)\n",
      "('is', 0.2126445520245001)\n",
      "('the', 0.12724213180694394)\n",
      "('all', 0.05644664752726577)\n",
      "('in', 0.0564466475272657)\n",
      "('singing', 0.0564466475272657)\n"
     ]
    }
   ],
   "source": [
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)  # zip of words and its values - we get tuples\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)  # sorting the tuples of components\n",
    "    sortedTerms = sortedTerms[:10] # 10 terms in specific concept\n",
    "    print(\"\\nConcept\",i,\":\")\n",
    "    for term in sortedTerms:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
